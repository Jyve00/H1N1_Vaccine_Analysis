{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "A one-paragraph overview of the project, including the business problem, data, methods, results and recommendations.\n",
    "\n",
    "To find potential factors in vaccine hesistancy in the COVID-19 pandemic, we used a dataset of characteristics of people who either got the H1N1 or didn't. We tested different machine learning models on the dataset for prediction accuracy and ended up choosing a ____ model.  \n",
    "\n",
    "## Business Problem\n",
    "\n",
    "Summary of the business problem you are trying to solve, and the data questions that you plan to answer to solve them.\n",
    "\n",
    "The CDC has hired us as data scientists to find lessons from the H1N1 pandemic. Early into the vaccine rollout, there has been massive public skepticism of the vaccine, slowing our ability to overcome the virus and re-open the country.\n",
    "\n",
    "We have received a large dataset from 2009, with statistical information and pandemic/H1N1 scaled opinions for each respondent. This data includes whether each person was vaccinated or not, which will allow us to predict who gets the vaccine.\n",
    "\n",
    "This model will allow the CDC to determine investments in public health awaareness, surveying and modelling during the pandemic, based on the parameters which impact an individuals vaccine choice the most.\n",
    "\n",
    "Questions to consider:\n",
    "\n",
    "- Who are your stakeholders?\n",
    "- What are your stakeholders' pain points related to this project?\n",
    "- Why are your predictions important from a business perspective?\n",
    "- What exactly is your deliverable: your analysis, or the model itself?\n",
    "- Does your business understanding/stakeholder require a specific type of model?\n",
    "    - For example: a highly regulated industry would require a very transparent/simple/interpretable model, whereas a situation where the model itself is your deliverable would likely benefit from a more complex and thus stronger model\n",
    "   \n",
    "\n",
    "Additional questions to consider for classification:\n",
    "\n",
    "- What does a false positive look like in this context?\n",
    "- What does a false negative look like in this context?\n",
    "- Which is worse for your stakeholder?\n",
    "- What metric are you focusing on optimizing, given the answers to the above questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "The dataset includes 26,000 respondents responses, and includes 34 different characteristics. They provide demographic information such as age, sex, race, income, and education and also include opinion and knowledge assessment on the risk of the H1N1 virus. \n",
    "\n",
    "\n",
    "The target variable is the 'h1n1 vaccine' column. It is binary: 0 means the respondent didn't get the vaccine and 1 means they did. \n",
    "\n",
    "Questions to consider:\n",
    "\n",
    "- Where did the data come from, and how do they relate to the data analysis questions?\n",
    "- What do the data represent? Who is in the sample and what variables are included?\n",
    "- What is the target variable?\n",
    "- What are the properties of the variables you intend to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 53414 entries, 0 to 26706\n",
      "Data columns (total 27 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   h1n1_concern                 26615 non-null  float64\n",
      " 1   h1n1_knowledge               26591 non-null  float64\n",
      " 2   behavioral_antiviral_meds    26636 non-null  float64\n",
      " 3   behavioral_avoidance         26499 non-null  float64\n",
      " 4   behavioral_face_mask         26688 non-null  float64\n",
      " 5   behavioral_wash_hands        26665 non-null  float64\n",
      " 6   behavioral_large_gatherings  26620 non-null  float64\n",
      " 7   behavioral_outside_home      26625 non-null  float64\n",
      " 8   behavioral_touch_face        26579 non-null  float64\n",
      " 9   doctor_recc_h1n1             24547 non-null  float64\n",
      " 10  chronic_med_condition        25736 non-null  float64\n",
      " 11  child_under_6_months         25887 non-null  float64\n",
      " 12  health_worker                25903 non-null  float64\n",
      " 13  opinion_h1n1_vacc_effective  26316 non-null  float64\n",
      " 14  opinion_h1n1_risk            26319 non-null  float64\n",
      " 15  opinion_h1n1_sick_from_vacc  26312 non-null  float64\n",
      " 16  age_group                    26707 non-null  object \n",
      " 17  education                    25300 non-null  object \n",
      " 18  race                         26707 non-null  object \n",
      " 19  sex                          26707 non-null  object \n",
      " 20  income_poverty               22284 non-null  object \n",
      " 21  marital_status               25299 non-null  object \n",
      " 22  rent_or_own                  24665 non-null  object \n",
      " 23  employment_status            25244 non-null  object \n",
      " 24  household_adults             26458 non-null  float64\n",
      " 25  household_children           26458 non-null  float64\n",
      " 26  h1n1_vaccine                 26707 non-null  float64\n",
      "dtypes: float64(19), object(8)\n",
      "memory usage: 11.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# code here to explore your data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('../Data/training_set_features.csv', index_col='respondent_id')\n",
    "target = pd.read_csv('../Data/training_set_labels.csv', index_col='respondent_id')\n",
    "holdout_set = pd.read_csv('../Data/test_set_features.csv', index_col='respondent_id')\n",
    "\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'feature_importances_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-fd66ea8e79ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mplot_feature_importances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-119-fd66ea8e79ee>\u001b[0m in \u001b[0;36mplot_feature_importances\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbarh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'center'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Feature importance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5138\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'feature_importances_'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lets see what features are the most important for our prediction. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Describe and justify the process for preparing the data for analysis.\n",
    "\n",
    "As the dataset also includes information on whether the respondent got the seasonal flu vaccine and their scaled opinions on the seasonal vaccine, we decided to drop all data and focus on h1n1-related data for model simplicity.\n",
    "\n",
    "We then decided to drop the health insurance column as there were many null values, and in initial data modelling our model was much more accurate with it gone.\n",
    "\n",
    "The data left is a combination of ordinal (>2 non-continuous) and binary variables, both in string and number format. We converted all string data to integers, i.e. 'Male' and 'Female' in the 'sex' column converted to 0 and 1, respectively.\n",
    "\n",
    "For remaining missing values in columns, we filled in the mode of the dataset. \n",
    "\n",
    "Questions to consider:\n",
    "\n",
    "- Were there variables you dropped or created?\n",
    "- How did you address missing values or outliers?\n",
    "- Why are these choices appropriate given the data and the business problem?\n",
    "- Can you pipeline your preparation steps to use them consistently in the modeling process?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'h1n1_vaccine')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dropping all columns that are related to the seasonal flue target\n",
    "data.drop(columns=[\n",
    "    'hhs_geo_region',\n",
    "    'census_msa', \n",
    "    'employment_industry', \n",
    "    'employment_occupation', \n",
    "    'opinion_seas_vacc_effective', \n",
    "    'opinion_seas_risk', \n",
    "    'opinion_seas_sick_from_vacc', \n",
    "    'health_insurance', \n",
    "    'doctor_recc_seasonal'], inplace=True)\n",
    "\n",
    "# We only need the 'h1n1_vaccine' column as our target variable so we'll turn it into a panda series with just that column. \n",
    "target = pd.Series(target['h1n1_vaccine'])\n",
    "\n",
    "# Visualize the target variable\n",
    "fig, ax = plt.subplots()\n",
    "target.value_counts().plot.barh(title=\"Proportion of H1N1 Vaccine\", ax=ax)\n",
    "ax.set_ylabel(\"h1n1_vaccine\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.787546\n",
       "1    0.212454\n",
       "Name: h1n1_vaccine, dtype: float64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# inspect the values \n",
    "target.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "We set up a Machine learning Pipeline using SkLearn in order to easily run different models without changing too much code. \n",
    "\n",
    "The first step in the Pipeline is the Preprossessing stage when we convert categorical data into none ordinal numbers and we will also scale the numeric columns. The preprosessing step also deals with missing values. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############\n",
    "Describe and justify the process for analyzing or modeling the data.\n",
    "\n",
    "Questions to consider:\n",
    "\n",
    "- How will you analyze the data to arrive at an initial approach?\n",
    "- How will you iterate on your initial approach to make it better?\n",
    "- What model type is most appropriate, given the data and the business problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'preprocessor', 'classifier', 'preprocessor__n_jobs', 'preprocessor__remainder', 'preprocessor__sparse_threshold', 'preprocessor__transformer_weights', 'preprocessor__transformers', 'preprocessor__verbose', 'preprocessor__numeric', 'preprocessor__cat', 'preprocessor__numeric__memory', 'preprocessor__numeric__steps', 'preprocessor__numeric__verbose', 'preprocessor__numeric__standard_scaler', 'preprocessor__numeric__simple_imputer', 'preprocessor__numeric__standard_scaler__copy', 'preprocessor__numeric__standard_scaler__with_mean', 'preprocessor__numeric__standard_scaler__with_std', 'preprocessor__numeric__simple_imputer__add_indicator', 'preprocessor__numeric__simple_imputer__copy', 'preprocessor__numeric__simple_imputer__fill_value', 'preprocessor__numeric__simple_imputer__missing_values', 'preprocessor__numeric__simple_imputer__strategy', 'preprocessor__numeric__simple_imputer__verbose', 'preprocessor__cat__memory', 'preprocessor__cat__steps', 'preprocessor__cat__verbose', 'preprocessor__cat__cat_imputer', 'preprocessor__cat__ohe', 'preprocessor__cat__cat_imputer__add_indicator', 'preprocessor__cat__cat_imputer__copy', 'preprocessor__cat__cat_imputer__fill_value', 'preprocessor__cat__cat_imputer__missing_values', 'preprocessor__cat__cat_imputer__strategy', 'preprocessor__cat__cat_imputer__verbose', 'preprocessor__cat__ohe__categories', 'preprocessor__cat__ohe__drop', 'preprocessor__cat__ohe__dtype', 'preprocessor__cat__ohe__handle_unknown', 'preprocessor__cat__ohe__sparse', 'classifier__C', 'classifier__class_weight', 'classifier__dual', 'classifier__fit_intercept', 'classifier__intercept_scaling', 'classifier__l1_ratio', 'classifier__max_iter', 'classifier__multi_class', 'classifier__n_jobs', 'classifier__penalty', 'classifier__random_state', 'classifier__solver', 'classifier__tol', 'classifier__verbose', 'classifier__warm_start'])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code inspired from https://www.drivendata.co/blog/predict-flu-vaccine-data-benchmark/\n",
    "\n",
    "# create lists containing names of related columns for transformation \n",
    "\n",
    "num_cols = data.select_dtypes('number').columns\n",
    "\n",
    "ord_cols = ['age_group', 'education',  'income_poverty', 'employment_status']\n",
    "\n",
    "cat_cols = ['race', 'sex', 'marital_status', 'rent_or_own'] \n",
    "\n",
    "\n",
    "# import neccessary libararies \n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\n",
    "\n",
    "# chain preprocessing into a Pipeline object\n",
    "# each step is a tuple of (name you chose, sklearn transformer)\n",
    "numeric_preprocessing_steps = Pipeline([\n",
    "    ('standard_scaler', StandardScaler()),\n",
    "    ('simple_imputer', SimpleImputer(strategy='median'))\n",
    "])\n",
    "\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    ('cat_imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),\n",
    "    ('ohe', OneHotEncoder(categories=\"auto\", handle_unknown='ignore'))])\n",
    "\n",
    "# create the preprocessor stage of final pipeline\n",
    "# each entry in the transformer list is a tuple of\n",
    "# (name you choose, sklearn transformer, list of columns)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers = [\n",
    "        (\"numeric\", numeric_preprocessing_steps, num_cols), \n",
    "        ('cat', cat_transformer, cat_cols)], remainder = \"drop\")\n",
    "\n",
    "\n",
    "# The Next step in the Pipeline is the prediction. We will use a Logistic Regression model to predict weather someone will get the H1N1 vaccine or not\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_pipeline = Pipeline(steps=[('preprocessor', preprocessor), \n",
    "        (\"classifier\", LogisticRegression())])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "log_pipeline.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# because this Pipeline has alot of hyperparamters we can tweak to try to get a better model we'll set up a Gridsearch to find the best parametrics that results in the best score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 2160 candidates, totalling 21600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done 496 tasks      | elapsed:   21.7s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:   36.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1136 tasks      | elapsed:   55.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1552 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2032 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3856 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 4592 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 5392 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 6256 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 7184 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=-1)]: Done 8176 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=-1)]: Done 9232 tasks      | elapsed:  7.9min\n",
      "[Parallel(n_jobs=-1)]: Done 10352 tasks      | elapsed:  8.8min\n",
      "[Parallel(n_jobs=-1)]: Done 11536 tasks      | elapsed:  9.8min\n",
      "[Parallel(n_jobs=-1)]: Done 12784 tasks      | elapsed: 10.8min\n",
      "[Parallel(n_jobs=-1)]: Done 14096 tasks      | elapsed: 11.9min\n",
      "[Parallel(n_jobs=-1)]: Done 15472 tasks      | elapsed: 13.0min\n",
      "[Parallel(n_jobs=-1)]: Done 16912 tasks      | elapsed: 14.3min\n",
      "[Parallel(n_jobs=-1)]: Done 18416 tasks      | elapsed: 15.7min\n",
      "[Parallel(n_jobs=-1)]: Done 19984 tasks      | elapsed: 17.0min\n",
      "[Parallel(n_jobs=-1)]: Done 21600 out of 21600 | elapsed: 18.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[('preprocessor',\n",
       "                                        ColumnTransformer(transformers=[('numeric',\n",
       "                                                                         Pipeline(steps=[('standard_scaler',\n",
       "                                                                                          StandardScaler()),\n",
       "                                                                                         ('simple_imputer',\n",
       "                                                                                          SimpleImputer(strategy='median'))]),\n",
       "                                                                         Index(['h1n1_concern', 'h1n1_knowledge', 'behavioral_antiviral_meds',\n",
       "       'behavioral_avoidance', 'behavioral_face_mask', 'behavioral_wash_hands',\n",
       "       'b...\n",
       "             param_grid={'classifier__C': [0.01, 0.02, 0.03, 0.04, 0.05, 0.06,\n",
       "                                           0.07, 0.08, 0.09, 0.1],\n",
       "                         'classifier__class_weight': ['balanced'],\n",
       "                         'classifier__max_iter': [25, 50, 75, 100, 125, 150],\n",
       "                         'classifier__multi_class': ['auto', 'ovr',\n",
       "                                                     'multinomial'],\n",
       "                         'classifier__penalty': ['l1', 'l2', 'elasticnet',\n",
       "                                                 None],\n",
       "                         'classifier__random_state': [42],\n",
       "                         'classifier__solver': ['lbfgs, ‘liblinear', 'sag',\n",
       "                                                'saga']},\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inprired by the code found at  https://towardsdatascience.com/ml-pipelines-with-grid-search-in-scikit-learn-2539d6b53cfb\n",
    "log_param = {\n",
    "        'classifier__penalty': ['l1', 'l2', 'elasticnet', None],                        # Method on how to deal with features with high Coefficient. \n",
    "        'classifier__C': [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.10],  # Setting regularization strength. \n",
    "        'classifier__max_iter': [25, 50, 75, 100, 125, 150],                            # Sets maximum number of itereations taken for solver to converge.\n",
    "        'classifier__multi_class': ['auto', 'ovr', 'multinomial'],                      # Sets how to define target class\n",
    "        'classifier__solver': ['lbfgs, ‘liblinear', 'sag', 'saga'],                     # Algorithm to use in the optimization problem\n",
    "        'classifier__class_weight': ['balanced'], \n",
    "        'classifier__random_state': [42]                                       # sets weights\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# we will now perform a train test split to make a terst set to run through the models\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_true, y_train, y_true = train_test_split(data, target, test_size=0.25, shuffle=True, stratify=target, random_state=42)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Initialize a Grid_Search object and pass thru the pipeline we set up and adjust the params we declared to be tweaked \n",
    "grid_search = GridSearchCV(estimator=log_pipeline, param_grid=log_param, cv=10, verbose=3,n_jobs=-1)\n",
    "# fit the grid_search model with the training data set\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 0.01,\n",
       " 'classifier__class_weight': 'balanced',\n",
       " 'classifier__max_iter': 25,\n",
       " 'classifier__multi_class': 'auto',\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__random_state': 42,\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "With Grid Search we can easily get a configuration for the best Parametrics to use on our model. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "////////////////////////////////////\n",
    "\n",
    "The evaluation of each model should accompany the creation of each model, and you should be sure to evaluate your models consistently.\n",
    "\n",
    "Evaluate how well your work solves the stated business problem. \n",
    "\n",
    "Questions to consider:\n",
    "\n",
    "- How do you interpret the results?\n",
    "- How well does your model fit your data? How much better is this than your baseline model? Is it over or under fit?\n",
    "- How well does your model/data fit any relevant modeling assumptions?\n",
    "\n",
    "For the final model, you might also consider:\n",
    "\n",
    "- How confident are you that your results would generalize beyond the data you have?\n",
    "- How confident are you that this model would benefit the business if put into use?\n",
    "- What does this final model tell you about the relationship between your inputs and outputs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Understanding\n",
    "\n",
    "- What does a baseline, model-less prediction look like?\n",
    "\n",
    "From a baseline understanding, the data set target is skewed towards those who didn't get the vaccine, 79% of respondents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First $&(@# Model\n",
    "\n",
    "Before going too far down the data preparation rabbit hole, be sure to check your work against a first 'substandard' model! What is the easiest way for you to find out how hard your problem is?\n",
    "\n",
    "We examined how accurately a decision tree-based model would be able to determine if someone got vaccinated or not based on their characteristics. \n",
    "\n",
    "In this case we dropped rows with missing values and used pd.get_dummies to split up the ordinal variables with 3 or more values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Iterations\n",
    "\n",
    "Now you can start to use the results of your first model to iterate - there are many options!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here to iteratively improve your models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here to evaluate your iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'Final' Model\n",
    "\n",
    "In the end, you'll arrive at a 'final' model - aka the one you'll use to make your recommendations/conclusions. This likely blends any group work. It might not be the one with the highest scores, but instead might be considered 'final' or 'best' for other reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here to show your final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here to evaluate your final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Provide your conclusions about the work you've done, including any limitations or next steps.\n",
    "\n",
    "Questions to consider:\n",
    "\n",
    "- What would you recommend the business do as a result of this work?\n",
    "- How could the stakeholder use your model effectively?\n",
    "- What are some reasons why your analysis might not fully solve the business problem?\n",
    "- What else could you do in the future to improve this project (future work)?\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "17cc554a2c16bc39ba98993395725f2ec0bd4b482a8c6c7a1f8b97bbfb728cd1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('learn-env': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
